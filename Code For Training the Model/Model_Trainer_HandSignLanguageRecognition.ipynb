{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Hand Sign Language Recognition Model Trainer - Google Colab Setup and Training**\n",
        "\n",
        "Upload and Run the Below Code in Google Collab\n",
        "\n"
      ],
      "metadata": {
        "id": "37zyhD6OOBFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vTC95oRITW3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### **Step 1: Install Required Packages**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DPE0G50rPeBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensure you have the necessary packages installed by running the following commands:\n"
      ],
      "metadata": {
        "id": "aLNdD0iIPg7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xtHICD_5q83"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### **Step 2: Import Required Libraries**"
      ],
      "metadata": {
        "id": "qi1uW8RoMU8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the required libraries and modules for the model training:"
      ],
      "metadata": {
        "id": "-ZPWquuePwjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15\n",
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xL2uEzge6Pjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### **Step 3: Mount Google Drive**"
      ],
      "metadata": {
        "id": "FF_lsGdiP1P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your Google Drive to access the dataset stored in your Google Drive's My Drive section:"
      ],
      "metadata": {
        "id": "Km-kvZfKMYS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SHdJsFh96T2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### **Step 4: Set the Path to your folder**"
      ],
      "metadata": {
        "id": "JG1CnjttMjhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_folder_path = '/content/drive/MyDrive/Projects/Dataset/Dataset_Hand_Sign_Language_Recognition'"
      ],
      "metadata": {
        "id": "rHk4x4Z67Auk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_folder_path)"
      ],
      "metadata": {
        "id": "iJV3pdge7cpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### **Step 5: Explore and Prepare the Dataset**"
      ],
      "metadata": {
        "id": "GypZiYJYQjRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_folder_path)\n",
        "labels = []\n",
        "for i in os.listdir(my_folder_path):\n",
        "  if os.path.isdir(os.path.join(my_folder_path, i)):\n",
        "    labels.append(i)\n",
        "for label in labels:\n",
        "  print(label + \"\\n\")"
      ],
      "metadata": {
        "id": "6-2-AjDi7fVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(labels))"
      ],
      "metadata": {
        "id": "kYNXFsIs7hiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EXAMPLES = 10\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(my_folder_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
        "    axs[i].get_xaxis().set_visible(False)\n",
        "    axs[i].get_yaxis().set_visible(False)\n",
        "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l04t0myT7m_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### **Step 6: Train the Hand Sign Language Recognition Model**"
      ],
      "metadata": {
        "id": "XjfjvJN3QsZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = gesture_recognizer.Dataset.from_folder(\n",
        "    dirname=my_folder_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        ")\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ],
      "metadata": {
        "id": "rwKrYGCa7ny6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")\n"
      ],
      "metadata": {
        "id": "Jv5PgJpu7pEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408a0591-4178-4c92-a14f-88f2cdd0a454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hand_embedding (InputLayer  [(None, 128)]             0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 128)               512       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " re_lu (ReLU)                (None, 128)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " custom_gesture_recognizer_  (None, 58)                7482      \n",
            " out (Dense)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7994 (31.23 KB)\n",
            "Trainable params: 7738 (30.23 KB)\n",
            "Non-trainable params: 256 (1.00 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "901/901 [==============================] - 6s 5ms/step - loss: 2.9731 - categorical_accuracy: 0.2614 - val_loss: 1.3607 - val_categorical_accuracy: 0.5644 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "901/901 [==============================] - 6s 6ms/step - loss: 1.7761 - categorical_accuracy: 0.4867 - val_loss: 0.9536 - val_categorical_accuracy: 0.6667 - lr: 9.9000e-04\n",
            "Epoch 3/10\n",
            "901/901 [==============================] - 4s 4ms/step - loss: 1.4359 - categorical_accuracy: 0.5666 - val_loss: 0.7983 - val_categorical_accuracy: 0.6933 - lr: 9.8010e-04\n",
            "Epoch 4/10\n",
            "901/901 [==============================] - 4s 5ms/step - loss: 1.2749 - categorical_accuracy: 0.6065 - val_loss: 0.7362 - val_categorical_accuracy: 0.7111 - lr: 9.7030e-04\n",
            "Epoch 5/10\n",
            "901/901 [==============================] - 6s 6ms/step - loss: 1.1550 - categorical_accuracy: 0.6332 - val_loss: 0.6932 - val_categorical_accuracy: 0.7244 - lr: 9.6060e-04\n",
            "Epoch 6/10\n",
            "901/901 [==============================] - 5s 5ms/step - loss: 1.0768 - categorical_accuracy: 0.6620 - val_loss: 0.6569 - val_categorical_accuracy: 0.7244 - lr: 9.5099e-04\n",
            "Epoch 7/10\n",
            "901/901 [==============================] - 6s 6ms/step - loss: 0.9990 - categorical_accuracy: 0.6715 - val_loss: 0.6356 - val_categorical_accuracy: 0.7600 - lr: 9.4148e-04\n",
            "Epoch 8/10\n",
            "901/901 [==============================] - 6s 7ms/step - loss: 0.9439 - categorical_accuracy: 0.6926 - val_loss: 0.6336 - val_categorical_accuracy: 0.7556 - lr: 9.3207e-04\n",
            "Epoch 9/10\n",
            "901/901 [==============================] - 4s 5ms/step - loss: 0.8997 - categorical_accuracy: 0.7048 - val_loss: 0.6298 - val_categorical_accuracy: 0.7556 - lr: 9.2274e-04\n",
            "Epoch 10/10\n",
            "901/901 [==============================] - 4s 5ms/step - loss: 0.8655 - categorical_accuracy: 0.7148 - val_loss: 0.6332 - val_categorical_accuracy: 0.7422 - lr: 9.1352e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ],
      "metadata": {
        "id": "xkvinDVZ7rkw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf708a2-9681-4eb8-997e-79ada47dcbee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226/226 [==============================] - 3s 4ms/step - loss: 0.5262 - categorical_accuracy: 0.7655\n",
            "Test loss:0.526222288608551, Test accuracy:0.7654867172241211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ],
      "metadata": {
        "id": "KUoZiBM07seK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917c9561-c58c-4683-d5b0-605e5203f895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://storage.googleapis.com/mediapipe-assets/gesture_embedder.tflite to /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
            "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
            "Downloading https://storage.googleapis.com/mediapipe-assets/canned_gesture_classifier.tflite to /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n",
            "best_model_weights.data-00000-of-00001\tcheckpoint    gesture_recognizer.task  metadata.json\n",
            "best_model_weights.index\t\tepoch_models  logs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### **Step 7: Download the Trained Model**"
      ],
      "metadata": {
        "id": "jtrpFRuEQ_gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ],
      "metadata": {
        "id": "jg1islqe7tWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bc289026-283f-487b-f411-30825aa24d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fe4be49a-41aa-4129-bd42-ba4d7b5c0905\", \"gesture_recognizer.task\", 8488926)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}